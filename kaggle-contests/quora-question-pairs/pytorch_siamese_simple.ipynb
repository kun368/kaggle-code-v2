{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bidict import bidict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.shared_code import MultiSummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class FastTextSiamese(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            emb_size: int = 100,\n",
    "            hidden_size: int = 10\n",
    "    ) -> None:\n",
    "        super(FastTextSiamese, self).__init__()\n",
    "        self.embeddings = nn.EmbeddingBag(vocab_size, emb_size, mode='mean')\n",
    "        self.hidden = nn.Sequential(nn.Linear(emb_size, hidden_size), nn.ReLU())\n",
    "        self.out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward_once(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self.embeddings(text)\n",
    "        return self.hidden(embeddings)\n",
    "\n",
    "    def forward(self, text1: torch.Tensor, text2: torch.Tensor) -> torch.Tensor:\n",
    "        out1 = self.forward_once(text1)\n",
    "        out2 = self.forward_once(text2)\n",
    "        dis = torch.abs(out1 - out2)\n",
    "        return self.out(dis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_parquet_folder(split):\n",
    "    p = \"D:/Code/Pycharm/kaggle-code-v2/kaggle-contests/quora-question-pairs/fasttext_data\"\n",
    "    return pd.concat(\n",
    "        [pd.read_parquet(f) for f in glob.glob(f\"{p}/{split}/*\") if str(f).endswith('.parquet')],\n",
    "        axis=0, ignore_index=True\n",
    "    )\n",
    "\n",
    "train_df, valid_df = train_test_split(read_parquet_folder('train_df'), test_size=0.2, shuffle=True)\n",
    "pred_df = read_parquet_folder('pred_df')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "sentence_word_limit = 30\n",
    "word_min_count = 3\n",
    "for ixd, i in train_df.iterrows():\n",
    "    w1 = nltk.word_tokenize(str(i['question1']))\n",
    "    w2 = nltk.word_tokenize(str(i['question2']))\n",
    "    word_counter.update(w1)\n",
    "    word_counter.update(w2)\n",
    "\n",
    "word_id_mapping = bidict(enumerate([k for k, v in word_counter.items() if v >= word_min_count]))\n",
    "word_id_mapping.inv['<pad>'] = len(word_id_mapping)\n",
    "word_id_mapping.inv['<unk>'] = len(word_id_mapping)\n",
    "\n",
    "def sentence_to_ids(text):\n",
    "    words = nltk.word_tokenize(str(text))[:sentence_word_limit]\n",
    "    words = [word_id_mapping.inv.get(i, word_id_mapping.inv['<unk>']) for i in words]\n",
    "    words += [word_id_mapping.inv['<pad>']] * (sentence_word_limit - len(words))\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df_dict = df.to_dict(orient='records')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur = self.df_dict[idx]\n",
    "        t1 = sentence_to_ids(cur['question1'])\n",
    "        t2 = sentence_to_ids(cur['question2'])\n",
    "        label = cur.get('is_duplicate', 0)\n",
    "        return torch.LongTensor(t1).to(device), torch.LongTensor(t2).to(device), torch.LongTensor([label]).to(device)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(QuoraDataset(train_df), batch_size, shuffle=False)\n",
    "valid_dl = DataLoader(QuoraDataset(valid_df), batch_size, shuffle=False)\n",
    "pred_dl = DataLoader(QuoraDataset(pred_df), batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 46291\n",
      "==================================================================\n",
      "                   Kernel Shape Output Shape   Params Mult-Adds\n",
      "Layer                                                          \n",
      "0_embeddings       [100, 46291]    [64, 100]  4.6291M   4.6291M\n",
      "1_hidden.Linear_0     [100, 10]     [64, 10]    1.01k      1.0k\n",
      "2_hidden.ReLU_1               -     [64, 10]        -         -\n",
      "3_embeddings       [100, 46291]    [64, 100]        -   4.6291M\n",
      "4_hidden.Linear_0     [100, 10]     [64, 10]        -      1.0k\n",
      "5_hidden.ReLU_1               -     [64, 10]        -         -\n",
      "6_out                   [10, 2]      [64, 2]     22.0      20.0\n",
      "------------------------------------------------------------------\n",
      "                         Totals\n",
      "Total params          4.630132M\n",
      "Trainable params      4.630132M\n",
      "Non-trainable params        0.0\n",
      "Mult-Adds              9.26022M\n",
      "==================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Pycharm\\ml_learning\\conda_env\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sum = df.sum()\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "\n",
    "vocab_size = len(word_id_mapping)\n",
    "print('vocab_size', vocab_size)\n",
    "sample = torch.zeros(size=(batch_size, sentence_word_limit), dtype=torch.int64)\n",
    "_ = summary(FastTextSiamese(vocab_size=vocab_size), x=sample, text2=sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5054 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ffc02d142944a3a88979775234bca91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1264 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc6c96e5281b43f9bb7219739b26551c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5054 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0035dedee41410a975fb1c0f79f5706"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1264 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3755a1b1aba8417cbc0c577228dde075"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5054 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e00917a67c3446848be7beb19f198d86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1264 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "887bea719630407fb0e8f1b374b9e4cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "net = FastTextSiamese(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, net.parameters()))\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "tot_epoch = 3\n",
    "for epoch in range(tot_epoch):\n",
    "    def run(data_loader, is_train=True):\n",
    "        net.train(is_train)\n",
    "        pbar = tqdm(iterable=data_loader, colour='#1d3557' if is_train else '#457b9d')\n",
    "        summer = MultiSummer()\n",
    "        for t1, t2, labels in pbar:\n",
    "            labels = torch.squeeze(labels)\n",
    "            y_hat = net(t1, t2)\n",
    "            loss = loss_func(y_hat, labels)\n",
    "            # metrics\n",
    "            local_real, local_pred = labels.detach().cpu(), np.argmax(y_hat.detach().cpu(), axis=1)\n",
    "            summer.put('loss', loss.item())\n",
    "            summer.put('precision', precision_score(local_real, local_pred, zero_division=0))\n",
    "            summer.put('recall', recall_score(local_real, local_pred, zero_division=0))\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            pbar.set_description(f'Epoch {epoch + 1}/{tot_epoch} - {summer}')\n",
    "\n",
    "\n",
    "    run(train_dl, is_train=True)\n",
    "    run(valid_dl, is_train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}